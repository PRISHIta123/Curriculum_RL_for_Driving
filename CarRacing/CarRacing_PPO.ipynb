{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336ca60d",
   "metadata": {},
   "source": [
    "**1. Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3696ab60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63dc7923",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO      #PPO -> Proximal Policy Optimization\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy  #to evaluate the model \n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common import type_aliases\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, VecMonitor, is_vecenv_wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e02a8d",
   "metadata": {},
   "source": [
    "**2.Train Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b14c40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Default environment\n",
    "# from gym.wrappers.time_limit import TimeLimit\n",
    "# from car_racing_default import CarRacingDefault\n",
    "# env = TimeLimit(CarRacingDefault(),max_episode_steps=1000)\n",
    "# eval_env= env\n",
    "\n",
    "# Curriculum environment\n",
    "# from gym.wrappers.time_limit import TimeLimit\n",
    "# from TurnRates.car_racing_curriculum import CarRacingCurriculum\n",
    "# env = TimeLimit(CarRacingCurriculum(),max_episode_steps=1000)\n",
    "\n",
    "# Evaluation environment\n",
    "# from gym.wrappers.time_limit import TimeLimit\n",
    "# from TurnRates.car_racing_eval import CarRacingEval\n",
    "# eval_env = TimeLimit(CarRacingEval(),max_episode_steps=1000)\n",
    "\n",
    "# Default obstacles environment\n",
    "# from gym.wrappers.time_limit import TimeLimit\n",
    "# from car_racing_obstacles import CarRacingObstacles\n",
    "# env = TimeLimit(CarRacingObstacles(),max_episode_steps=1000)\n",
    "# eval_env= env\n",
    "\n",
    "# Curriculum obstacles environment\n",
    "# from gym.wrappers.time_limit import TimeLimit\n",
    "# from Obstacles.car_racing_obstacles_curriculum import CarRacingObstaclesCurriculum\n",
    "# env = TimeLimit(CarRacingObstaclesCurriculum(),max_episode_steps=1000)\n",
    "\n",
    "# Obstacles Evaluation environment\n",
    "from gym.wrappers.time_limit import TimeLimit\n",
    "from Obstacles.car_racing_obstacles_eval import CarRacingObstaclesEval\n",
    "eval_env = TimeLimit(CarRacingObstaclesEval(),max_episode_steps=1000)\n",
    "\n",
    "# Curriculum both environment\n",
    "# from gym.wrappers.time_limit import TimeLimit\n",
    "# from Both.car_racing_obstacles_curriculum_both import CarRacingObstaclesCurriculumBoth\n",
    "# env = TimeLimit(CarRacingObstaclesCurriculumBoth(),max_episode_steps=1000)\n",
    "\n",
    "# Evaluation both environment\n",
    "# from gym.wrappers.time_limit import TimeLimit\n",
    "# from Both.car_racing_obstacles_eval_both import CarRacingObstaclesEvalBoth\n",
    "# eval_env = TimeLimit(CarRacingObstaclesEvalBoth(),max_episode_steps=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c289c",
   "metadata": {},
   "source": [
    "**3.Training Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028e8252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd13e99c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PrishitaRay1/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x15f549100> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x15f5717c0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "/Users/PrishitaRay1/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Eval num_timesteps=50000, episode_reward=-4.97 +/- 37.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 7\n",
      "Eval num_timesteps=100000, episode_reward=-1.79 +/- 44.80\n",
      "Episode length: 920.60 +/- 238.20\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 4\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Eval num_timesteps=150000, episode_reward=16.38 +/- 60.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 12\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 9\n",
      "Eval num_timesteps=200000, episode_reward=272.01 +/- 165.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 9\n",
      "Eval num_timesteps=250000, episode_reward=182.40 +/- 57.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 4\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Eval num_timesteps=300000, episode_reward=233.66 +/- 140.99\n",
      "Episode length: 988.70 +/- 33.90\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 11\n",
      "Eval num_timesteps=350000, episode_reward=281.34 +/- 165.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 8\n",
      "Eval num_timesteps=400000, episode_reward=102.01 +/- 171.18\n",
      "Episode length: 646.10 +/- 171.38\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Eval num_timesteps=450000, episode_reward=291.01 +/- 112.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 11\n",
      "Eval num_timesteps=500000, episode_reward=262.59 +/- 159.50\n",
      "Episode length: 808.80 +/- 212.14\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 4\n",
      "Total number of obstacles in the track: 6\n",
      "Eval num_timesteps=550000, episode_reward=546.86 +/- 187.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 4\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 6\n",
      "Eval num_timesteps=600000, episode_reward=-16.21 +/- 59.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 4\n",
      "Eval num_timesteps=650000, episode_reward=605.79 +/- 153.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 4\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 9\n",
      "Eval num_timesteps=700000, episode_reward=431.64 +/- 197.19\n",
      "Episode length: 970.60 +/- 88.20\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Eval num_timesteps=750000, episode_reward=682.80 +/- 130.24\n",
      "Episode length: 964.80 +/- 54.54\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Eval num_timesteps=800000, episode_reward=419.57 +/- 275.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 4\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Eval num_timesteps=850000, episode_reward=596.10 +/- 152.72\n",
      "Episode length: 970.10 +/- 89.70\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 5\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 10\n",
      "Eval num_timesteps=900000, episode_reward=701.09 +/- 81.17\n",
      "Episode length: 988.30 +/- 35.10\n",
      "New best mean reward!\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 6\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 5\n",
      "Eval num_timesteps=950000, episode_reward=446.54 +/- 111.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 7\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 9\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 8\n",
      "Total number of obstacles in the track: 11\n",
      "Total number of obstacles in the track: 10\n",
      "Total number of obstacles in the track: 9\n",
      "Eval num_timesteps=1000000, episode_reward=180.99 +/- 48.55\n",
      "Episode length: 1000.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('./runs')\n",
    "\n",
    "# Learning Rates: \n",
    "# 0.0005- PPO Default, 0.00025- PPO Curriculum\n",
    "# 0.0005- PPO Obstacles, 0.000475- PPO Obstacles Curriculum\n",
    "# 0.0002- PPO Both Curriculum\n",
    "\n",
    "model = PPO('CnnPolicy', env, learning_rate=0.000475, n_steps=1000, batch_size=1000, verbose=0, seed=0, tensorboard_log=log_path)\n",
    "ppo_path = os.path.join('./Training/Saved_Models/PPO_car_best_Model_curriculum_obstacles1')\n",
    "\n",
    "eval_callback = EvalCallback(eval_env=eval_env, best_model_save_path=ppo_path,\n",
    "                              n_eval_episodes=10,\n",
    "                             eval_freq=50000,verbose=1,\n",
    "                             deterministic=True, render=False)\n",
    "model.learn(total_timesteps=1000000,callback=eval_callback)\n",
    "ppo_path = os.path.join('./Training/Saved_Models/PPO_Model_final_curriculum_obstacles1.zip')\n",
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b24f5c",
   "metadata": {},
   "source": [
    "**4.Evaluating Models**- Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd500cf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppo_path = os.path.join('./Training/Saved_Models/PPO_car_best_Model_curriculum_obstacles/best_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31709fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = PPO.load(ppo_path, env=eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76eeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate_policy(\n",
    "    model: \"type_aliases.PolicyPredictor\",\n",
    "    env: Union[gym.Env, VecEnv],\n",
    "    n_eval_episodes: int = 10,\n",
    "    deterministic: bool = True,\n",
    "    render: bool = False,\n",
    "    callback: Optional[Callable[[Dict[str, Any], Dict[str, Any]], None]] = None,\n",
    "    reward_threshold: Optional[float] = None,\n",
    "    return_episode_rewards: bool = False,\n",
    "    warn: bool = True,\n",
    ") -> Union[Tuple[float, float], Tuple[List[float], List[int]]]:\n",
    "    \"\"\"\n",
    "    Runs policy for ``n_eval_episodes`` episodes and returns average reward.\n",
    "    If a vector env is passed in, this divides the episodes to evaluate onto the\n",
    "    different elements of the vector env. This static division of work is done to\n",
    "    remove bias. See https://github.com/DLR-RM/stable-baselines3/issues/402 for more\n",
    "    details and discussion.\n",
    "\n",
    "    .. note::\n",
    "        If environment has not been wrapped with ``Monitor`` wrapper, reward and\n",
    "        episode lengths are counted as it appears with ``env.step`` calls. If\n",
    "        the environment contains wrappers that modify rewards or episode lengths\n",
    "        (e.g. reward scaling, early episode reset), these will affect the evaluation\n",
    "        results as well. You can avoid this by wrapping environment with ``Monitor``\n",
    "        wrapper before anything else.\n",
    "\n",
    "    :param model: The RL agent you want to evaluate. This can be any object\n",
    "        that implements a `predict` method, such as an RL algorithm (``BaseAlgorithm``)\n",
    "        or policy (``BasePolicy``).\n",
    "    :param env: The gym environment or ``VecEnv`` environment.\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :param render: Whether to render the environment or not\n",
    "    :param callback: callback function to do additional checks,\n",
    "        called after each step. Gets locals() and globals() passed as parameters.\n",
    "    :param reward_threshold: Minimum expected reward per episode,\n",
    "        this will raise an error if the performance is not met\n",
    "    :param return_episode_rewards: If True, a list of rewards and episode lengths\n",
    "        per episode will be returned instead of the mean.\n",
    "    :param warn: If True (default), warns user about lack of a Monitor wrapper in the\n",
    "        evaluation environment.\n",
    "    :return: Mean reward per episode, std of reward per episode.\n",
    "        Returns ([float], [int]) when ``return_episode_rewards`` is True, first\n",
    "        list containing per-episode rewards and second containing per-episode lengths\n",
    "        (in number of steps).\n",
    "    \"\"\"\n",
    "    is_monitor_wrapped = False\n",
    "    # Avoid circular import\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "    if not isinstance(env, VecEnv):\n",
    "        env = DummyVecEnv([lambda: env])  # type: ignore[list-item, return-value]\n",
    "\n",
    "    is_monitor_wrapped = is_vecenv_wrapped(env, VecMonitor) or env.env_is_wrapped(Monitor)[0]\n",
    "\n",
    "    if not is_monitor_wrapped and warn:\n",
    "        warnings.warn(\n",
    "            \"Evaluation environment is not wrapped with a ``Monitor`` wrapper. \"\n",
    "            \"This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. \"\n",
    "            \"Consider wrapping environment first with ``Monitor`` wrapper.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "\n",
    "    n_envs = env.num_envs\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    episode_collisions = []\n",
    "    episode_collision_ratios = []\n",
    "    episode_tiles = []\n",
    "    episode_grass_ratios = []\n",
    "\n",
    "    episode_counts = np.zeros(n_envs, dtype=\"int\")\n",
    "    # Divides episodes among different sub environments in the vector as evenly as possible\n",
    "    episode_count_targets = np.array([(n_eval_episodes + i) // n_envs for i in range(n_envs)], dtype=\"int\")\n",
    "\n",
    "    current_rewards = np.zeros(n_envs)\n",
    "    current_lengths = np.zeros(n_envs, dtype=\"int\")\n",
    "    observations = env.reset()\n",
    "    states = None\n",
    "    episode_starts = np.ones((env.num_envs,), dtype=bool)\n",
    "    while (episode_counts < episode_count_targets).any():\n",
    "        actions, states = model.predict(\n",
    "            observations,  # type: ignore[arg-type]\n",
    "            state=states,\n",
    "            episode_start=episode_starts,\n",
    "            deterministic=deterministic,\n",
    "        )\n",
    "        new_observations, rewards, dones, infos = env.step(actions)\n",
    "        current_rewards += rewards\n",
    "        current_lengths += 1\n",
    "        for i in range(n_envs):\n",
    "            if episode_counts[i] < episode_count_targets[i]:\n",
    "                # unpack values so that the callback can access the local variables\n",
    "                reward = rewards[i]\n",
    "                done = dones[i]\n",
    "                info = infos[i]\n",
    "                episode_starts[i] = done\n",
    "\n",
    "                if callback is not None:\n",
    "                    callback(locals(), globals())\n",
    "\n",
    "                if dones[i]:\n",
    "                    if is_monitor_wrapped:\n",
    "                        # Atari wrapper can send a \"done\" signal when\n",
    "                        # the agent loses a life, but it does not correspond\n",
    "                        # to the true end of episode\n",
    "                        if \"episode\" in info.keys():\n",
    "                            # Do not trust \"done\" with episode endings.\n",
    "                            # Monitor wrapper includes \"episode\" key in info if environment\n",
    "                            # has been wrapped with it. Use those rewards instead.\n",
    "                            episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "                            episode_lengths.append(info[\"episode\"][\"l\"])\n",
    "                    \n",
    "                            # Only increment at the real end of an episode\n",
    "                            episode_counts[i] += 1\n",
    "                    else:\n",
    "                        episode_rewards.append(current_rewards[i])\n",
    "                        episode_lengths.append(current_lengths[i])\n",
    "                        \n",
    "                        if \"num_obstacles\" in infos[i].keys():\n",
    "                            episode_collisions.append(infos[i][\"num_collisions\"])\n",
    "                            episode_collision_ratios.append(infos[i][\"num_collisions\"]/infos[i][\"num_obstacles\"])\n",
    "                        \n",
    "                        episode_tiles.append(infos[i][\"tiles\"])\n",
    "                        episode_grass_ratios.append(infos[i][\"grass_time\"]/infos[i][\"total_time\"])\n",
    "                        \n",
    "                        episode_counts[i] += 1\n",
    "                    current_rewards[i] = 0\n",
    "                    current_lengths[i] = 0\n",
    "\n",
    "        observations = new_observations\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    mean_collisions = np.mean(episode_collisions)\n",
    "    mean_collision_ratios = np.mean(episode_collision_ratios)\n",
    "    mean_tiles = np.mean(episode_tiles)\n",
    "    mean_grass_ratios = np.mean(episode_grass_ratios)\n",
    "    \n",
    "    if reward_threshold is not None:\n",
    "        assert mean_reward > reward_threshold, \"Mean reward below threshold: \" f\"{mean_reward:.2f} < {reward_threshold:.2f}\"\n",
    "    if return_episode_rewards:\n",
    "        return episode_rewards, episode_lengths\n",
    "    return mean_reward, std_reward, mean_collisions, mean_collision_ratios, mean_tiles, mean_grass_ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb43d45b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5r/58zq17s164z4sxr5htkjbnch0000gp/T/ipykernel_23148/1992896862.py:57: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(663.8577228378505,\n",
       " 133.57166080533986,\n",
       " 0.932,\n",
       " 0.12012632922632924,\n",
       " 236.172,\n",
       " 0.026208400941482225)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalue = custom_evaluate_policy(best_model, eval_env, n_eval_episodes=500, render = False)\n",
    "eval_env.close()\n",
    "evalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928044c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
